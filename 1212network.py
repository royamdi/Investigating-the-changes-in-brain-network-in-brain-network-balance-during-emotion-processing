# -*- coding: utf-8 -*-
"""1212NETWORK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H-JxUMfQxwl727vm3R4v2mMPR-7gnf5n
"""

import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# The data shared for NMA projects is a subset of the full HCP dataset
N_SUBJECTS = 100

# The data have already been aggregated into ROIs from the Glasser parcellation
N_PARCELS = 360

# The acquisition parameters for all tasks were identical
TR = 0.72  # Time resolution, in seconds

# The parcels are matched across hemispheres with the same order
HEMIS = ["Right", "Left"]

# Each experiment was repeated twice in each subject
RUNS   = ['LR','RL']
N_RUNS = 2

# There are 7 tasks. Each has a number of 'conditions'
# TIP: look inside the data folders for more fine-graned conditions

EXPERIMENTS = {

    'EMOTION'    : {'cond':['fear','neut']},

}

# @title Download data file
import os, requests

fname = "hcp_task.tgz"
url = "https://osf.io/2y3fw/download"

if not os.path.isfile(fname):
  try:
    r = requests.get(url)
  except requests.ConnectionError:
    print("!!! Failed to download data !!!")
  else:
    if r.status_code != requests.codes.ok:
      print("!!! Failed to download data !!!")
    else:
      with open(fname, "wb") as fid:
        fid.write(r.content)

# The download cells will store the data in nested directories starting here:
HCP_DIR = "./hcp_task"

# importing the "tarfile" module
import tarfile

# open file
with tarfile.open(fname) as tfile:
  # extracting file
  tfile.extractall('.')

subjects = np.loadtxt(os.path.join(HCP_DIR, 'subjects_list.txt'), dtype='str')

regions = np.load(f"{HCP_DIR}/regions.npy").T
region_info = dict(
    name=regions[0].tolist(),
    network=regions[1],
    hemi=['Right']*int(N_PARCELS/2) + ['Left']*int(N_PARCELS/2),
)

def load_single_timeseries(subject, experiment, run, remove_mean=True):
  """Load timeseries data for a single subject and single run.

  Args:
    subject (str):      subject ID to load
    experiment (str):   Name of experiment
    run (int):          (0 or 1)
    remove_mean (bool): If True, subtract the parcel-wise mean (typically the mean BOLD signal is not of interest)

  Returns
    ts (n_parcel x n_timepoint array): Array of BOLD data values

  """
  bold_run  = RUNS[run]
  bold_path = f"{HCP_DIR}/subjects/{subject}/{experiment}/tfMRI_{experiment}_{bold_run}"
  bold_file = "data.npy"
  ts = np.load(f"{bold_path}/{bold_file}")
  if remove_mean:
    ts -= ts.mean(axis=1, keepdims=True)
  return ts


def load_evs(subject, experiment, run):
  """Load EVs (explanatory variables) data for one task experiment.

  Args:
    subject (str): subject ID to load
    experiment (str) : Name of experiment
    run (int): 0 or 1

  Returns
    evs (list of lists): A list of frames associated with each condition

  """
  frames_list = []
  task_key = f'tfMRI_{experiment}_{RUNS[run]}'
  for cond in EXPERIMENTS[experiment]['cond']:
    ev_file  = f"{HCP_DIR}/subjects/{subject}/{experiment}/{task_key}/EVs/{cond}.txt"
    ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)
    ev       = dict(zip(["onset", "duration", "amplitude"], ev_array))
    # Determine when trial starts, rounded down
    start = np.floor(ev["onset"] / TR).astype(int)
    # Use trial duration to determine how many frames to include for trial
    duration = np.ceil(ev["duration"] / TR).astype(int)
    # Take the range of frames that correspond to this specific trial
    frames = [s + np.arange(0, d) for s, d in zip(start, duration)]
    frames_list.append(frames)

  return frames_list

my_exp = 'EMOTION'
my_subj = subjects[1]
my_run = 1

data = load_single_timeseries(subject=my_subj,
                              experiment=my_exp,
                              run=my_run,
                              remove_mean=True)
print(data.shape)

evs = load_evs(subject=my_subj, experiment=my_exp, run=my_run)

#Load time series data:
ts = load_single_timeseries(my_subj, my_exp, my_run)

networks = np.unique(region_info['network'])

networks

network_ts = {}
for network in networks:
  idx = (region_info['network'] == network)
  network_ts[network] = data[idx].mean(axis=1)

# Get frames for each condition
fear_frames = evs[0]
neut_frames = evs[1]

fear_frames = np.concatenate(evs[0])
fear_frames = fear_frames[fear_frames < 176]

neut_frames = np.concatenate(evs[1])
neut_frames = neut_frames[neut_frames < 176]

fear_ts = data[:, fear_frames]
neut_ts = data[:, neut_frames]

# Calculate mean time series for each network
fear_network_ts = {}
neut_network_ts = {}

for network in networks:
  idx = (region_info['network'] == network)

  fear_network_ts[network] = fear_ts[idx].mean(axis=0)
  neut_network_ts[network] = neut_ts[idx].mean(axis=0)

# Compute connectivity between networks
fear_network_corr = np.zeros((len(networks), len(networks)))
neut_network_corr = np.zeros((len(networks), len(networks)))

from scipy.stats import pearsonr

for i, net1 in enumerate(networks):
  for j, net2 in enumerate(networks):

    fear_network_corr[i,j] = pearsonr(fear_network_ts[net1], fear_network_ts[net2])[0]
    neut_network_corr[i,j] = pearsonr(neut_network_ts[net1], neut_network_ts[net2])[0]

fear_network_corr

neut_network_corr

np.shape(fear_network_corr)

np.shape(neut_network_corr)

plt.imshow(fear_network_corr)
plt.colorbar()
plt.title('Fear Condition Connectivity')
plt.xlabel('Parcel')
plt.ylabel('Parcel')
plt.show()

plt.imshow(neut_network_corr)
plt.colorbar()
plt.title('Fear Condition Connectivity')
plt.xlabel('Parcel')
plt.ylabel('Parcel')
plt.show()

def count_triads(conn):
    n_nodes = conn.shape[0]
    n_balanced = 0
    n_imbalanced = 0

    for i in range(n_nodes):
        for j in range(i+1, n_nodes):
            for k in range(j+1, n_nodes):
                triad = np.sign(conn[i,j]) * np.sign(conn[j,k]) * np.sign(conn[k,i])
                if triad > 0:
                    n_balanced += 1
                elif triad < 0:
                    n_imbalanced += 1

    return n_balanced, n_imbalanced

n_balanced_fear, n_imbalanced_fear = count_triads(fear_network_corr)
n_balanced_neut, n_imbalanced_neut = count_triads(neut_network_corr)

print("Fear:")
print("Balanced:", n_balanced_fear)
print("Imbalanced:", n_imbalanced_fear)

print("Neutral:")
print("Balanced:", n_balanced_neut)
print("Imbalanced:", n_imbalanced_neut)

balance_energy_fear = n_balanced_fear / (n_balanced_fear + n_imbalanced_fear)
balance_energy_neut = n_balanced_neut / (n_balanced_neut + n_imbalanced_neut)

print("Balance energy:")
print("Fear:", balance_energy_fear)
print("Neutral:", balance_energy_neut)

from tqdm import tqdm

def get_network_connectivity(timeseries_data, networks, region_info):

  network_ts = {}

  for network in networks:
    idx = (region_info['network'] == network)
    network_ts[network] = timeseries_data[idx].mean(axis=0)

  network_corr = np.zeros((len(networks), len(networks)))

  for i, net1 in enumerate(networks):
    for j, net2 in enumerate(networks):
       network_corr[i,j] = pearsonr(network_ts[net1], network_ts[net2])[0]

  return network_corr

N_SUBJECTS = 100
n_balanced_fear = np.zeros((N_SUBJECTS,))
n_imbalanced_fear = np.zeros((N_SUBJECTS,))
balance_energy_fear = np.zeros((N_SUBJECTS,))

n_balanced_neut = np.zeros((N_SUBJECTS,))
n_imbalanced_neut = np.zeros((N_SUBJECTS,))
balance_energy_neut = np.zeros((N_SUBJECTS,))

for i in tqdm(range(N_SUBJECTS)):

    # Load data for subject i
    ts = load_single_timeseries(subjects[i], my_exp, my_run)
    fear_frames = load_evs(subjects[i], my_exp, my_run)[0]
    neut_frames = load_evs(subjects[i], my_exp, my_run)[1]

    fear_frames = np.concatenate(evs[0])
    fear_frames = fear_frames[fear_frames < 176]

    neut_frames = np.concatenate(evs[1])
    neut_frames = neut_frames[neut_frames < 176]
    fear_ts = data[:, fear_frames]
    neut_ts = data[:, neut_frames]

    # Calculate connectivity matrices
    fear_network_corr = get_network_connectivity(fear_ts, networks, region_info)
    neut_network_corr = get_network_connectivity(neut_ts, networks, region_info)

    # Calculate triad counts
    n_balanced_fear[i], n_imbalanced_fear[i] = count_triads(fear_network_corr)
    n_balanced_neut[i], n_imbalanced_neut[i] = count_triads(neut_network_corr)

    # Calculate balance energy
    balance_energy_fear[i] = n_balanced_fear[i] / (n_balanced_fear[i] + n_imbalanced_fear[i])
    balance_energy_neut[i] = n_balanced_neut[i] / (n_balanced_neut[i] + n_imbalanced_neut[i])

pip install xlsxwriter

import xlsxwriter

# Authenticate drive
from google.colab import drive
drive.mount('/content/drive')

workbook = xlsxwriter.Workbook('/content/drive/MyDrive/balance_result121212.xlsx')
worksheet = workbook.add_worksheet()
# Write headers
worksheet.write(0, 0, 'Subject')
worksheet.write(0, 1, 'n_balanced_fear')
worksheet.write(0, 2, 'n_balanced_neut')
worksheet.write(0, 3, 'balance_energy_fear')
worksheet.write(0, 4, 'balance_energy_neut')

# Loop through subjects and write data
for i in range(len(n_balanced_fear)):
  worksheet.write(i+1, 0, i+1)
  worksheet.write(i+1, 1, n_balanced_fear[i])
  worksheet.write(i+1, 2, n_balanced_neut[i])
  worksheet.write(i+1, 3, balance_energy_fear[i])
  worksheet.write(i+1, 4, balance_energy_neut[i])

workbook.close()

from scipy.stats import ttest_rel

balance_energy_diff = balance_energy_fear - balance_energy_neut

t, p = ttest_rel(balance_energy_fear, balance_energy_neut)

print("t-statistic:", t)
print("p-value:", p)

if p < 0.05:
  print("Difference is significant at p<0.05")
else:
  print("Difference is not significant at p<0.05")