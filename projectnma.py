# -*- coding: utf-8 -*-
"""ProjectNMA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aUjMzQyBTA-0AX0g7IA-GB6BUBOoJaqh
"""

import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# number of subjects
N_SUBJECTS = 100

# The data have already been aggregated into ROIs from the Glasser parcellation
N_PARCELS = 360

# Temporal resolution , second
TR = 0.72

# The parcels are matched across hemispheres with the same order
HEMIS = ["Right", "Left"]

# Each experiment was repeated twice in each subject
RUNS   = ['LR','RL']
N_RUNS = 2

#EXP
EXPERIMENTS = { 'EMOTION'    : {'cond':['fear','neut']} }

# Download data file
import os, requests

fname = "hcp_task.tgz"
url = "https://osf.io/2y3fw/download"

if not os.path.isfile(fname):
  try:
    r = requests.get(url)
  except requests.ConnectionError:
    print("!!! Failed to download data !!!")
  else:
    if r.status_code != requests.codes.ok:
      print("!!! Failed to download data !!!")
    else:
      with open(fname, "wb") as fid:
        fid.write(r.content)

# The download cells will store the data in nested directories starting here:
HCP_DIR = "./hcp_task"

# importing the "tarfile" module
import tarfile

# open file
with tarfile.open(fname) as tfile:
  # extracting file
  tfile.extractall('.')
  subjects = np.loadtxt(os.path.join(HCP_DIR, 'subjects_list.txt'), dtype='str')

"""## Understanding the folder organisation

The data folder has the following organisation:

- hcp
  - regions.npy (information on the brain parcellation)
  - subjects_list.txt (list of subject IDs)
  - subjects (main data folder)
    - [subjectID] (subject-specific subfolder)
      - EXPERIMENT (one folder per experiment)
        - RUN (one folder per run)
          - data.npy (the parcellated time series data)
          - EVs (EVs folder)
            - [ev1.txt] (one file per condition)
            - [ev2.txt]
            - Stats.txt (behavioural data [where available] - averaged per run)
            - Sync.txt (ignore this file)


"""

#loading region

regions = np.load(f"{HCP_DIR}/regions.npy").T
region_info = dict(
    name=regions[0].tolist(),
    network=regions[1],
    hemi=['Right']*int(N_PARCELS/2) + ['Left']*int(N_PARCELS/2),
)

def load_single_timeseries(subject, experiment, run, remove_mean=True):
  """Load timeseries data for a single subject and single run.

  Args:
    subject (str):      subject ID to load
    experiment (str):   Name of experiment
    run (int):          (0 or 1)
    remove_mean (bool): If True, subtract the parcel-wise mean (typically the mean BOLD signal is not of interest)

  Returns
    ts (n_parcel x n_timepoint array): Array of BOLD data values

  """
  bold_run  = RUNS[run]
  bold_path = f"{HCP_DIR}/subjects/{subject}/{experiment}/tfMRI_{experiment}_{bold_run}"
  bold_file = "data.npy"
  ts = np.load(f"{bold_path}/{bold_file}")
  if remove_mean:
    ts -= ts.mean(axis=1, keepdims=True)
  return ts

def load_evs(subject, experiment, run):
  """Load EVs (explanatory variables) data for one task experiment.

  Args:
    subject (str): subject ID to load
    experiment (str) : Name of experiment
    run (int): 0 or 1

  Returns
    evs (list of lists): A list of frames associated with each condition

  """
  frames_list = []
  task_key = f'tfMRI_{experiment}_{RUNS[run]}'
  for cond in EXPERIMENTS[experiment]['cond']:
    ev_file  = f"{HCP_DIR}/subjects/{subject}/{experiment}/{task_key}/EVs/{cond}.txt"
    ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)
    ev       = dict(zip(["onset", "duration", "amplitude"], ev_array))
    # Determine when trial starts, rounded down
    start = np.floor(ev["onset"] / TR).astype(int)
    # Use trial duration to determine how many frames to include for trial
    duration = np.ceil(ev["duration"] / TR).astype(int)
    # Take the range of frames that correspond to this specific trial
    frames = [s + np.arange(0, d) for s, d in zip(start, duration)]
    frames_list.append(frames)

  return frames_list

#event_information

my_exp = 'EMOTION'
my_subj = subjects[1]
my_run = 1
evs = load_evs(subject=my_subj, experiment=my_exp, run=my_run)

evs

#Load time series data:
ts = load_single_timeseries(my_subj, my_exp, my_run)

"""The goal is to calculate functional connectivity matrices for the fear and neutral conditions.

First, it gets the frame indices corresponding to trials for each condition from the evs variable.

It initializes empty connectivity matrices for fear (conn_fear) and neutral (conn_neut).

It clips the frame indices to valid values based on the number of timepoints in ts. This avoids indexing errors.

Then it loops through each pair of parcels (i and j) to calculate connectivity:

For each condition, it initializes an empty list to store correlations (fcorr and ncorr).
It loops through the frames for that condition.
For each frame, it calculates the Pearson correlation between the timeseries of parcel i and parcel j.
It stores this correlation value in the appropriate list.
After looping through all frames, it takes the average of the correlations and stores it in the connectivity matrix at index [i,j].
So in summary, it is calculating the average functional connectivity (Pearson correlation) between each pair of parcels, within the frames corresponding to each experimental condition. This results in a 360x360 matrix capturing the average connectivity network for fear and neutral.

The connectivity values indicate how synchronized the BOLD activity is between each pair of brain regions during each condition. This can then be compared between conditions to see how functional connectivity changes with emotional state.


"""

#Get frame indices for conditions:
fear_frames = evs[0]
neut_frames = evs[1]

"""This grabs the list of frame indices for the fear and neutral conditions from the evs variable. evs[0] contains the frames for fear trials, evs[1] contains frames for neutral trials."""

#Initialize connectivity matrices:
conn_fear = np.zeros((N_PARCELS, N_PARCELS))
conn_neut = np.zeros((N_PARCELS, N_PARCELS))

"""This creates two 360x360 arrays filled with zeros, to store the connectivity matrices."""

# Clip frame indices
max_frames = ts.shape[1]
fear_frames = [f[f < max_frames] for f in fear_frames]
neut_frames = [f[f < max_frames] for f in neut_frames]

# Calculate connectivity
for i in range(N_PARCELS):
  for j in range(N_PARCELS):

    fcorr = []
    for f in fear_frames:
      corr = np.corrcoef(ts[i, f], ts[j, f])[0,1]
      fcorr.append(corr)

    conn_fear[i, j] = np.mean(fcorr)

    ncorr = []
    for f in neut_frames:
       corr = np.corrcoef(ts[i, f], ts[j, f])[0,1]
       ncorr.append(corr)

    conn_neut[i, j] = np.mean(ncorr)

"""first gets the max number of timepoints in the timeseries data ts, and clips the frame indices to be valid values below that maximum. This avoids indexing errors later.

*Loops through each parcel pair i, j

*For each condition:

  *Initialize empty list to store correlations: fcorr, ncorr

  *Loop through frames

   **Calculate correlation between parcel timeseries i and j
   

**Append to appropriate list

  *Take average of correlations and store in connectivity matrix at [i,j]

This does the main connectivity calculation, looping through conditions, frames, and parcel pairs to calculate the correlations and store the averages in the matrices.
"""

print(type(conn_fear))
print(conn_fear.shape)

print(type(conn_neut))
print(conn_neut.shape)

conn_fear

conn_neut

import matplotlib.pyplot as plt

plt.imshow(conn_fear)
plt.colorbar()
plt.title('Fear Condition Connectivity')
plt.xlabel('Parcel')
plt.ylabel('Parcel')
plt.show()

plt.imshow(conn_neut)
plt.colorbar()
plt.title('Neut Condition Connectivity')
plt.xlabel('Parcel')
plt.ylabel('Parcel')
plt.show()

import numpy as np

def count_triads(conn):
    n_nodes = conn.shape[0]
    n_balanced = 0
    n_imbalanced = 0

    for i in range(n_nodes):
        for j in range(i+1, n_nodes):
            for k in range(j+1, n_nodes):
                triad = np.sign(conn[i,j]) * np.sign(conn[j,k]) * np.sign(conn[k,i])
                if triad > 0:
                    n_balanced += 1
                elif triad < 0:
                    n_imbalanced += 1

    return n_balanced, n_imbalanced

"""This function is used to count the number of balanced and imbalanced triads in a connectivity matrix based on structural balance theory. Here is what it is doing:

Get the number of nodes (brain regions) from the matrix shape
Initialize counters for balanced and imbalanced triads to 0
Loop through all possible triads:
Loop through node i
Loop through nodes j > i (to avoid duplicates)
Loop through nodes k > j
For each triad ijk:
Calculate the "triad sign" by multiplying the connectivity values: sign(i,j) * sign(j,k) * sign(k,i)
If triad sign is positive, increment balanced counter
If triad sign is negative, increment imbalanced counter
Return final counts of balanced and imbalanced triads
So in summary, it enumerates all possible 3-node subgroups from the connectivity matrix, classifies them based on the sign of their connections, and tallies up the balanced vs imbalanced triad counts.

This allows comparing the level of "structural balance" between different conditions, as measured by the ratio of balanced to imbalanced triads.
"""

n_balanced_fear, n_imbalanced_fear = count_triads(conn_fear)
n_balanced_neut, n_imbalanced_neut = count_triads(conn_neut)

print("Fear:")
print("Balanced:", n_balanced_fear)
print("Imbalanced:", n_imbalanced_fear)

print("Neutral:")
print("Balanced:", n_balanced_neut)
print("Imbalanced:", n_imbalanced_neut)

balance_energy_fear = n_balanced_fear / (n_balanced_fear + n_imbalanced_fear)
balance_energy_neut = n_balanced_neut / (n_balanced_neut + n_imbalanced_neut)

print("Balance energy:")
print("Fear:", balance_energy_fear)
print("Neutral:", balance_energy_neut)

"""Based on the results, we can see that there are more balanced triads and higher balance energy in the neutral condition compared to the fear condition.

A few key observations:

- There are more balanced triads and fewer imbalanced triads in the neutral connectivity matrix compared to the fear matrix. This indicates the connections have more "structural balance" in the neutral state.

- The balance energy is higher for neutral (0.64) than fear (0.62). The balance energy quantifies the overall level of structural balance, with 1 being fully balanced and 0 being fully imbalanced. So the neutral state shows more balance.

- However, both conditions show balance energy > 0.5, meaning there are more balanced than imbalanced triads. This is expected in brain networks.

- The differences between fear and neutral are relatively small in terms of raw triad counts and balance energy. But the consistent pattern suggests the fear condition may disturb the balance of functional connections slightly.

In summary, the neutral state appears to have more balanced functional connectivity overall, while fear induces a slight shift towards more imbalance. This aligns with the theory that negative emotions like fear can perturb brain network dynamics. But more analysis would be needed to further test this hypothesis.
"""

